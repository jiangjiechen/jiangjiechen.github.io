
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Jiangjie Chen (ÈôàÊ±üÊç∑) is a researcher at ByteDance Seed Team. In 2024, he earned his Ph.D. at Fudan University in the School of Computer Science, Shanghai, China. His current interested research topics are mostly around building reasoning models and autonomous agents:\nReasoning Models: Advancing research on incentivizing and understanding advanced reasoning and planning capabilities from large models. Autonomous Agents: Developing advanced methods for autonomous, trustworthy, and personalized agents. This extends towards the exploration of their interactions with multiple agents and real environments. ","date":1748304000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1748304000,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Jiangjie Chen (ÈôàÊ±üÊç∑) is a researcher at ByteDance Seed Team. In 2024, he earned his Ph.D. at Fudan University in the School of Computer Science, Shanghai, China. His current interested research topics are mostly around building reasoning models and autonomous agents:","tags":null,"title":"Jiangjie Chen","type":"authors"},{"authors":["Jiangjie Chen","Qianyu He","Siyu Yuan","Aili Chen","Zhicheng Cai","Weinan Dai","Hongli Yu","Qiying Yu","Xuefeng Li","Jiaze Chen","Hao Zhou","Mingxuan Wang"],"categories":null,"content":"","date":1748304000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1748304000,"objectID":"50ea5c099420e84b582c6267908c1380","permalink":"https://jiangjiechen.github.io/publication/enigmata/","publishdate":"2025-05-27T00:00:00Z","relpermalink":"/publication/enigmata/","section":"publication","summary":"We introduce Enigmata, the first comprehensive suite tailored for improving LLMs with puzzle reasoning skills.","tags":["Large Language Models","Reinforcement Learning","Reasoning","Large Reasoning Models"],"title":"Enigmata: Scaling Logical Reasoning in Large Language Models with Synthetic Verifiable Puzzles","type":"publication"},{"authors":["ByteDance Seed"],"categories":null,"content":"","date":1744243200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1744243200,"objectID":"50620d7ae2a9d95a382259c8cba0f761","permalink":"https://jiangjiechen.github.io/publication/seed_thinking_v1.5/","publishdate":"2025-04-10T00:00:00Z","relpermalink":"/publication/seed_thinking_v1.5/","section":"publication","summary":"We introduce Seed-Thinking-v1.5, a Mixture-of-Experts (MoE) model with a relatively small size, featuring 20B activated and 200B total parameters, capable of reasoning through thinking before responding, resulting in improved performance on a widerange of benchmarks.","tags":["Large Language Models","Reinforcement Learning","Reasoning","Large Reasoning Models"],"title":"Seed-Thinking-v1.5: Advancing Superb Reasoning Models with Reinforcement Learning","type":"publication"},{"authors":["Qiying Yu","Zheng Zhang","Ruofei Zhu","Yufeng Yuan","Xiaochen Zuo","Yu Yue","Tiantian Fan","Gaohong Liu","Lingjun Liu","Xin Liu","Haibin Lin","Zhiqi Lin","Bole Ma","Guangming Sheng","Yuxuan Tong","Chi Zhang","Mofan Zhang","Wang Zhang","Hang Zhu","Jinhua Zhu","Jiaze Chen","Jiangjie Chen","Chengyi Wang","Hongli Yu","Weinan Dai","Yuxuan Song","Xiangpeng Wei","Hao Zhou","Jingjing Liu","Wei-Ying Ma","Ya-Qin Zhang","Lin Yan","Mu Qiao","Yonghui Wu","Mingxuan Wang"],"categories":null,"content":"","date":1743033600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1743033600,"objectID":"eef607d30ab2f76368cd82c5a6cbc437","permalink":"https://jiangjiechen.github.io/publication/dapo/","publishdate":"2025-03-27T00:00:00Z","relpermalink":"/publication/dapo/","section":"publication","summary":"We introduce DAPO, a Decoupled Clip and Dynamic sAmpling Policy Optimization algorithm, and fully open-source a state-of-the-art large-scale RL system that achieves 50 points on AIME 2024 using Qwen2.5-32B base model.","tags":["Large Language Models","Reinforcement Learning","Reasoning","Large Reasoning Models"],"title":"DAPO: An Open-source LLM Reinforcement Learning System At Scale","type":"publication"},{"authors":["Lida Chen","Dong Xu","Chenxin An","Xintao Wang","Yikai Zhang","Jiangjie Chen","Zujie Liang","Feng Wei","Jiaqing Liang","Yanghua Xiao","Wei Wang"],"categories":null,"content":"","date":1741132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1741132800,"objectID":"f8434c6f077fba2936a548443cf8c129","permalink":"https://jiangjiechen.github.io/publication/powerattention/","publishdate":"2025-03-05T00:00:00Z","relpermalink":"/publication/powerattention/","section":"publication","summary":"We introduce PowerAttention, a novel sparse attention design that facilitates effective and complete context extension through the theoretical analysis.","tags":["Large Language Models","Sparse Attention","Long-Context Models"],"title":"PowerAttention: Exponentially Scaling of Receptive Fields for Effective Sparse Attention","type":"publication"},{"authors":["Aili Chen","Chengyu Du","Jiangjie Chen","Jinghan Xu","Yikai Zhang","Siyu Yuan","Zulong Chen","Liangyue Li","Yanghua Xiao"],"categories":null,"content":"","date":1739664000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1739664000,"objectID":"48ae997383a17833fc065c0b5d4be674","permalink":"https://jiangjiechen.github.io/publication/deeper/","publishdate":"2025-02-16T00:00:00Z","relpermalink":"/publication/deeper/","section":"publication","summary":"We introduce DEEPER, a novel approach for dynamic persona modeling that enables continual persona optimization.","tags":["Large Language Models","Recommendation","Role-Playing Agent"],"title":"DEEPER Insight into Your User: Directed Persona Refinement for Dynamic Persona Modeling","type":"publication"},{"authors":["Xintao Wang","Heng Wang","Yifei Zhang","Xinfeng Yuan","Rui Xu","Jen-tse Huang","Siyu Yuan","Haoran Guo","Jiangjie Chen","Wei Wang","Yanghua Xiao","Shuchang Zhou"],"categories":null,"content":"","date":1739404800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1739404800,"objectID":"fbf278abc38fd01d9fee2e758af0b783","permalink":"https://jiangjiechen.github.io/publication/coser/","publishdate":"2025-02-13T00:00:00Z","relpermalink":"/publication/coser/","section":"publication","summary":"We introduce CoSER, a collection of a high-quality dataset, open models, and an evaluation protocol towards effective RPLAs of established characters.","tags":["Large Language Models","Role-Playing Agent"],"title":"CoSER: Coordinating LLM-Based Persona Simulation of Established Roles","type":"publication"},{"authors":["Siyu Yuan","Kaitao Song","Jiangjie Chen","Xu Tan","Yongliang Shen","Ren Kan","Dongsheng Li","Deqing Yang"],"categories":null,"content":"","date":1737590400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737590400,"objectID":"30a2c2d2fef66f08ca3d727f1b3bcb60","permalink":"https://jiangjiechen.github.io/publication/easytool/","publishdate":"2024-01-15T00:00:00Z","relpermalink":"/publication/easytool/","section":"publication","summary":"We proposes EASYTOOL, a method that simplifies tool documentation into concise instructions, improving tool use by language models.","tags":["Tool-Use","Agent","Large Language Models","Retrieval"],"title":"EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction","type":"publication"},{"authors":["Siyu Yuan","Kaitao Song","Jiangjie Chen","Xu Tan","Dongsheng Li","Deqing Yang"],"categories":null,"content":"","date":1737590400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737590400,"objectID":"b696eab8bac4e5195c90cfbb076061cb","permalink":"https://jiangjiechen.github.io/publication/evoagent/","publishdate":"2024-06-15T00:00:00Z","relpermalink":"/publication/evoagent/","section":"publication","summary":"We introduce EvoAgent, a method using evolutionary algorithms to automatically expand expert agents into multi-agent systems, enhancing the task-solving capabilities of large language model-based agents without additional human design.","tags":["Planning","Agent","Large Language Models","Reasoning"],"title":"EvoAgent: Towards Automatic Multi-Agent Generation via Evolutionary Algorithms","type":"publication"},{"authors":["Jian Xie","Kexun Zhang","Jiangjie Chen","Siyu Yuan","Kai Zhang","Yikai Zhang","Lei Li","Yanghua Xiao"],"categories":null,"content":"","date":1737590400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737590400,"objectID":"f19624e83b526669b61c05ee3e1578c2","permalink":"https://jiangjiechen.github.io/publication/barrierinplanning/","publishdate":"2025-01-23T00:00:00Z","relpermalink":"/publication/barrierinplanning/","section":"publication","summary":"We reveal the two key factors that hinder language agents from achieving human-level planning.","tags":["Planning","Agent","Large Language Models"],"title":"Revealing the Barriers of Language Agents in Planning","type":"publication"},{"authors":["Ruihan Yang","Jiangjie Chen","Yikai Zhang","Siyu Yuan","Aili Chen","Kyle Richardson","Yanghua Xiao","Deqing Yang"],"categories":null,"content":"","date":1737590400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1737590400,"objectID":"715be1bf9717a354b37188530c63824f","permalink":"https://jiangjiechen.github.io/publication/selfgoal/","publishdate":"2024-06-10T00:00:00Z","relpermalink":"/publication/selfgoal/","section":"publication","summary":"We introduce SelfGoal, an automatic approach that enhances language agents' capabilities to achieve high-level goals with limited instructions and delayed feedback by adaptively breaking down goals into practical subgoals.","tags":["Large Language Models","Planning","Agent"],"title":"SelfGoal: Your Language Agents Already Know How to Achieve High-level Goals","type":"publication"},{"authors":["Wei Shi","Shuang Li","Kerun Yu","Jinglei Chen","Zujie Liang","Xinhui Wu","Yuxi Qian","Feng Wei","Bo Zheng","Jiaqing Liang","Jiangjie Chen","Yanghua Xiao"],"categories":null,"content":"","date":1728345600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1728345600,"objectID":"ccca2578f3c840cf0371d4f82b4635b7","permalink":"https://jiangjiechen.github.io/publication/segmentplus/","publishdate":"2024-10-08T00:00:00Z","relpermalink":"/publication/segmentplus/","section":"publication","summary":"We introduce SEGMENT+, a framework that enables LMs to efficiently handle extended inputs within limited context windows, improving performance in long-document tasks through structured notes and a filtering module.","tags":["Large Language Models","Long-Context"],"title":"SEGMENT+: Long Text Processing with Short-Context Language Models","type":"publication"},{"authors":["Nianqi Li","Siyu Yuan","Jiangjie Chen","Jiaqing Liang","Feng Wei","Zujie Liang","Deqing Yang","Yanghua Xiao"],"categories":null,"content":"","date":1727136000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1727136000,"objectID":"6ad5340c868bc77312a25749d717d8c8","permalink":"https://jiangjiechen.github.io/publication/historyanalogy/","publishdate":"2024-09-24T00:00:00Z","relpermalink":"/publication/historyanalogy/","section":"publication","summary":"We focus on acquiring historical analogies using LLMs, proposing a self-reflection method to reduce hallucinations and stereotypes, showing that LLMs have strong potential in this task.","tags":["Large Language Models","Analogical Reasoning","Dataset","Reasoning"],"title":"Past Meets Present: Creating Historical Analogy with Large Language Models","type":"publication"},{"authors":["Aili Chen","Xuyang Ge","Ziquan Fu","Yanghua Xiao","Jiangjie Chen"],"categories":null,"content":"","date":1726099200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1726099200,"objectID":"65c7813ad9f78783c01fb978327b4db3","permalink":"https://jiangjiechen.github.io/publication/travelagent/","publishdate":"2024-09-12T00:00:00Z","relpermalink":"/publication/travelagent/","section":"publication","summary":"We introduce TravelAgent, an LLM-powered travel planning system that generates rational, comprehensive, and personalized itineraries through four modules, demonstrating effectiveness in dynamic scenarios.","tags":["Large Language Models","Agent","Planning"],"title":"TravelAgent: An AI Assistant for Personalized Travel Planning","type":"publication"},{"authors":["Siye Wu","Jian Xie","Jiangjie Chen","Tinghui Zhu","Kai Zhang","Yanghua Xiao"],"categories":null,"content":"","date":1720569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1720569600,"objectID":"70b734dea3c4873864438bd64c1c5112","permalink":"https://jiangjiechen.github.io/publication/irrelevant/","publishdate":"2024-07-10T00:00:00Z","relpermalink":"/publication/irrelevant/","section":"publication","summary":"We study how LLMs handle irrelevant information and find they struggle with content that is semantically related but ultimately not pertinent, highlighting the limitations of current systems in filtering out such distractions.","tags":["Large Language Models","LLM Analysis","Retrieval"],"title":"How Easily do Irrelevant Inputs Skew the Responses of Large Language Models?","type":"publication"},{"authors":["Zhouhong Gu","Lin Zhang","Xiaoxuan Zhu","Jiangjie Chen","Wenhao Huang","Yikai Zhang","Shusen Wang","Zheyu Ye","Yan Gao","Hongwei Feng","Yanghua Xiao"],"categories":null,"content":"","date":1718409600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1718409600,"objectID":"5cfd7c2df4a752504609206898a756bb","permalink":"https://jiangjiechen.github.io/publication/detectbench/","publishdate":"2024-06-15T00:00:00Z","relpermalink":"/publication/detectbench/","section":"publication","summary":"We introduce DetectBench, a benchmark for testing LLMs' evidence detection in long contexts, and demonstrates that while existing LLMs lag behind human performance, the proposed Detective Reasoning Prompt and Finetuning methods can significantly improve their evidence detection and reasoning capabilities.","tags":["Reasoning","Benchmark","Large Language Models","Resources"],"title":"DetectBench: Can Large Language Model Detect and Piece Together Implicit Evidence?","type":"publication"},{"authors":["Xinfeng Yuan","Siyu Yuan","Yuhan Cui","Tianhe Lin","Xintao Wang","Rui Xu","Jiangjie Chen","Deqing Yang"],"categories":null,"content":"","date":1717286400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1717286400,"objectID":"62f90522fbb793f5842bb90e8013c020","permalink":"https://jiangjiechen.github.io/publication/cross/","publishdate":"2024-06-02T00:00:00Z","relpermalink":"/publication/cross/","section":"publication","summary":"We propose evaluating large language models' character understanding through character profiling, using the CroSS dataset and showing promising results for role-playing agent development.","tags":["Large Language Models","Agent","Dataset","Role-Playing Agent"],"title":"Evaluating Character Understanding of Large Language Models via Character Profiling from Fictional Works","type":"publication"},{"authors":["Jiangjie Chen","Xintao Wang","Rui Xu","Siyu Yuan","Yikai Zhang","Wei Shi","Jian Xie","Shuang Li","Ruihan Yang","Tinghui Zhu","Aili Chen","Nianqi Li","Lida Chen","Caiyu Hu","Siye Wu","Scott Ren","Ziquan Fu","Yanghua Xiao"],"categories":null,"content":"","date":1714435200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714435200,"objectID":"67793c9dea68de81261facea75758193","permalink":"https://jiangjiechen.github.io/publication/rplasurvey/","publishdate":"2024-04-30T00:00:00Z","relpermalink":"/publication/rplasurvey/","section":"publication","summary":"This paper surveys Role-Playing Language Agents (RPLAs) by categorizing personas, discussing their development, and examining their applications, challenges, and future directions.","tags":["Survey","Role-Playing Agent","Large Language Models","Agent"],"title":"From Persona to Personalization: A Survey on Role-Playing Language Agents","type":"publication"},{"authors":["Rui Xu","Xintao Wang","Jiangjie Chen","Siyu Yuan","Xinfeng Yuan","Jiaqing Liang","Zulong Chen","Xiaoqing Dong","Yanghua Xiao"],"categories":null,"content":"","date":1713571200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1713571200,"objectID":"9ea3ca063b1ec56fc5065cd4e592eff7","permalink":"https://jiangjiechen.github.io/publication/lifechoice/","publishdate":"2024-04-20T00:00:00Z","relpermalink":"/publication/lifechoice/","section":"publication","summary":"We evaluate the potential of LLMs to make decisions as literary characters, using a new dataset and improved method that enhances decision-making accuracy, with future work and resources to be shared publicly.","tags":["Large Language Models","Agent","Dataset","Role-Playing Agent"],"title":"Character is Destiny: Can Large Language Models Simulate Persona-Driven Decisions in Role-Playing?","type":"publication"},{"authors":["Xintao Wang","Jiangjie Chen","Nianqi Li","Lida Chen","Xinfeng Yuan","Wei Shi","Xuyang Ge","Rui Xu","Yanghua Xiao"],"categories":null,"content":"","date":1712707200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1712707200,"objectID":"cc62e6fd219244bd48438aa40d4830b0","permalink":"https://jiangjiechen.github.io/publication/surveyagent/","publishdate":"2024-04-10T00:00:00Z","relpermalink":"/publication/surveyagent/","section":"publication","summary":"We propose a novel conversational AI system that enhances researchers' literature review processes by providing personalized knowledge management, literature recommendations, and query answering through a unified platform.","tags":["Large Language Models","Agent"],"title":"SurveyAgent: A Conversational System for Personalized and Efficient Research Survey","type":"publication"},{"authors":["Zhouhong Gu","Xiaoxuan Zhu","Haoran Guo","Lin Zhang","Yin Cai","Hao Shen","Jiangjie Chen","Zheyu Ye","Yifei Dai","Yan Gao","Yao Hu","Hongwei Feng","Yanghua Xiao"],"categories":null,"content":"","date":1710892800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1710892800,"objectID":"1505cee2a7722645562fde89df04e9a8","permalink":"https://jiangjiechen.github.io/publication/agentgroupchat/","publishdate":"2024-03-20T00:00:00Z","relpermalink":"/publication/agentgroupchat/","section":"publication","summary":"We propose a simulation to study language's influence on collective behavior by having agents engage in free chat within various narrative scenarios, with findings suggesting that greater information exchange promotes more orderly and meaningful emergent behaviors.","tags":["Agent","LLM Analysis","Simulated Environment","Large Language Models"],"title":"Agent Group Chat: An Interactive Group Chat Simulacra For Better Eliciting Collective Emergent Behavior","type":"publication"},{"authors":["Yikai Zhang","Siyu Yuan","Caiyu Hu","Kyle Richardson","Yanghua Xiao","Jiangjie Chen"],"categories":null,"content":"","date":1708041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1708041600,"objectID":"2e81269e317f3355721db3bdb0134c2c","permalink":"https://jiangjiechen.github.io/publication/timearena/","publishdate":"2024-02-16T00:00:00Z","relpermalink":"/publication/timearena/","section":"publication","summary":"TimeArena enhances LLMs with temporal dynamics for better multitasking, showing advanced models like GPT-4 still trail behind human temporal awareness.","tags":["Large Language Models","Planning","Agent","Benchmark","Simulated Environment"],"title":"TimeArena: Shaping Efficient Multitasking Language Agents in a Time-Aware Simulation","type":"publication"},{"authors":["Xintao Wang","Yunze Xiao","Jen-tse Huang","Siyu Yuan","Rui Xu","Haoran Guo","Quan Tu","Yaying Fei","Ziang Leng","Wei Wang","Jiangjie Chen","Cheng Li","Yanghua Xiao"],"categories":null,"content":"","date":1707955200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1707955200,"objectID":"b4378676134b9a9161c3057e55e599c4","permalink":"https://jiangjiechen.github.io/publication/incharacter/","publishdate":"2024-02-15T00:00:00Z","relpermalink":"/publication/incharacter/","section":"publication","summary":"We propose InCharacter, a method using psychological scales to evaluate the personality fidelity of role-playing agents (RPAs) powered by large language models.","tags":["Large Language Models","Role-Playing Agent","Agent"],"title":"InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews","type":"publication"},{"authors":["Jiayi Fu","Xuandong Zhao","Ruihan Yang","Yuansen Zhang","Jiangjie Chen","Yanghua Xiao"],"categories":null,"content":"","date":1707868800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1707868800,"objectID":"a5cb68b89e2b4c7be934dc022feac181","permalink":"https://jiangjiechen.github.io/publication/gumbelsoft/","publishdate":"2024-02-14T00:00:00Z","relpermalink":"/publication/gumbelsoft/","section":"publication","summary":"We propose GumbelSoft to improve the diversity of text outputs from LLMs while maintaining high detectability, outperforming other watermarking methods in both aspects.","tags":["Large Language Models","Watermarking","Diversified Generation"],"title":"GumbelSoft: Diversified Language Model Watermarking via the GumbelMax-trick","type":"publication"},{"authors":["Jian Xie","Kai Zhang","Jiangjie Chen","Tinghui Zhu","Renze Lou","Yuandong Tian","Yanghua Xiao","Yu Su"],"categories":null,"content":"","date":1707004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1707004800,"objectID":"0f5cd26b09ec0ad05a49eeba64143cb9","permalink":"https://jiangjiechen.github.io/publication/travelplanner/","publishdate":"2024-02-04T00:00:00Z","relpermalink":"/publication/travelplanner/","section":"publication","summary":"We introduced TravelPlanner, a benchmark for assessing language agents' planning abilities, showing that even advanced models like GPT-4 face difficulties with complex tasks.","tags":["Large Language Models","Planning","Agent","Benchmark","Tool-Use","Simulated Environment"],"title":"TravelPlanner: A Benchmark for Real-World Planning with Language Agents","type":"publication"},{"authors":["Jiangjie Chen","Siyu Yuan","Rong Ye","Bodhisattwa Prasad Majumder","Kyle Richardson"],"categories":null,"content":"","date":1696896000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696896000,"objectID":"eb162672168ca28ecab0c71c36a926e5","permalink":"https://jiangjiechen.github.io/publication/aucarena/","publishdate":"2023-10-10T00:00:00Z","relpermalink":"/publication/aucarena/","section":"publication","summary":"We propose AucArena to tests LLMs in auctions, showing they can strategize but with variable success, indicating potential for enhancement.","tags":["Reasoning","Planning","Agent","LLM Analysis","Simulated Environment","Large Language Models","Benchmark"],"title":"Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and Execution of LLM Agents in an Auction Arena","type":"publication"},{"authors":["Shuang Li","Jiangjie Chen","Siyu Yuan","Xinyi Wu","Hao Yang","Shimin Tao","Yanghua Xiao"],"categories":null,"content":"","date":1692576000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692576000,"objectID":"dfaa2c04032c12f1ed9f0e5ab0685755","permalink":"https://jiangjiechen.github.io/publication/idiomkb/","publishdate":"2023-08-21T00:00:00Z","relpermalink":"/publication/idiomkb/","section":"publication","summary":"We propose a multilingual idiom KB (IdiomKB) developed using LLMs to facilitate better idiomatic translation by smaller models by retrieving idioms‚Äô figurative meanings.","tags":["Machine Translation","Large Language Models","Dataset","Constrained Generation","Retrieval"],"title":"Translate Meanings, Not Just Words: IdiomKB's Role in Optimizing Idiomatic Translation with Language Models","type":"publication"},{"authors":["Jian Xie","Kai Zhang","Jiangjie Chen","Renze Lou","Yu Su"],"categories":null,"content":"","date":1684886400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684886400,"objectID":"0d69072ca97e42cce515ea9f015b0006","permalink":"https://jiangjiechen.github.io/publication/sloth/","publishdate":"2023-05-24T00:00:00Z","relpermalink":"/publication/sloth/","section":"publication","summary":"We present the first comprehensive and controlled investigation into the behavior of large language models when encountering knowledge conflicts.","tags":["Large Language Models","LLM Analysis","Retrieval"],"title":"Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts","type":"publication"},{"authors":["Siyu Yuan","Jiangjie Chen","Xuyang Ge","Yanghua Xiao","Deqing Yang"],"categories":null,"content":"","date":1684627200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684627200,"objectID":"0e990e9bf5e064adbbf6e10b0a4e6b5c","permalink":"https://jiangjiechen.github.io/publication/scar/","publishdate":"2023-05-21T00:00:00Z","relpermalink":"/publication/scar/","section":"publication","summary":"We propose a scientific analogical reasoning benchmark with structure abduction, SCAR, and show that large language models make reasonable scientific analogies after structure abduction.","tags":["Analogical Reasoning","Dataset","Large Language Models","Reasoning","LLM Analysis","Explainable NLP","Benchmark"],"title":"Beneath Surface Similarity: Large Language Models Make Reasonable Scientific Analogies after Structure Abduction","type":"publication"},{"authors":["Siyu Yuan","Jiangjie Chen","Changzhi Sun","Jiaqing Liang","Yanghua Xiao","Deqing Yang"],"categories":null,"content":"","date":1684022400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684022400,"objectID":"9b645b154c27bfd0ca5580f5234fadab","permalink":"https://jiangjiechen.github.io/publication/analogykb/","publishdate":"2023-05-14T00:00:00Z","relpermalink":"/publication/analogykb/","section":"publication","summary":"A million-scale analogy KB derived from existing KGs, to enable large language models to achieve analogical reasoning skills.","tags":["Analogical Reasoning","Dataset","Large Language Models","Reasoning"],"title":"AnalogyKB: Unlocking Analogical Reasoning of Language Models with A Million-scale Knowledge Base","type":"publication"},{"authors":["Siyu Yuan","Jiangjie Chen","Ziquan Fu","Xuyang Ge","Soham Shah","Charles Robert Jankowski","Yanghua Xiao","Deqing Yang"],"categories":null,"content":"","date":1682985600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682985600,"objectID":"e62dc6ae58c077e1ce8c14bd663cf183","permalink":"https://jiangjiechen.github.io/publication/coscript/","publishdate":"2023-05-02T00:00:00Z","relpermalink":"/publication/coscript/","section":"publication","summary":"We propose an over-generate-then-filter approach to improve large language models (LLMs) on constrained language planning, and use it to distill a novel constrained language planning dataset, CoScript.","tags":["Planning","Dataset","Large Language Models","Constrained Generation","Benchmark"],"title":"Distilling Script Knowledge from Large Language Models for Constrained Language Planning","type":"publication"},{"authors":["Jiangjie Chen","Wei Shi","Ziquan Fu","Sijie Cheng","Lei Li","Yanghua Xiao"],"categories":null,"content":"","date":1682985600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682985600,"objectID":"92becb6ae0e6f580cda428a3c3d8a85d","permalink":"https://jiangjiechen.github.io/publication/uncommongen/","publishdate":"2023-05-02T00:00:00Z","relpermalink":"/publication/uncommongen/","section":"publication","summary":"We find that large language models (LLMs) speak too positively about negative commonsense knowledge, which is caused by statistical shortcuts and negation reporting bias from language modeling pre-training.","tags":["Large Language Models","Constrained Generation","Reasoning","LLM Analysis","Benchmark"],"title":"Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge","type":"publication"},{"authors":["Jiangjie Chen","Yanghua Xiao"],"categories":null,"content":"","date":1669939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669939200,"objectID":"8124f02540e4f2a1f249e2d3eab7a289","permalink":"https://jiangjiechen.github.io/publication/tcdereview/","publishdate":"2022-12-02T00:00:00Z","relpermalink":"/publication/tcdereview/","section":"publication","summary":"We briefly review the recent progress of knowledge-guided NLG, sets ten goals for future development, and envisions challenges in attaining these objectives.","tags":["Text Generation","Reasoning"],"title":"Harnessing Knowledge and Reasoning for Human-Like Natural Language Generation: A Brief Review","type":"publication"},{"authors":["Jiangjie Chen","Rui Xu","Wenxuan Zeng","Changzhi Sun","Lei Li","Yanghua Xiao"],"categories":null,"content":"","date":1669075200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669075200,"objectID":"0c1332d33b542e55786ed9851f315ce5","permalink":"https://jiangjiechen.github.io/publication/vence/","publishdate":"2022-11-22T00:00:00Z","relpermalink":"/publication/vence/","section":"publication","summary":"Fact verification guided iterative factual error correction without the supervision from corretion.","tags":["Text Generation","Faithfulness and Factuality","Constrained Generation","Retrieval"],"title":"Converge to the Truth: Factual Error Correction via Iterative Constrained Editing","type":"publication"},{"authors":["Sijie Cheng","Zhiyong Wu","Jiangjie Chen","Zhixing Li","Yang Liu","Lingpeng Kong"],"categories":null,"content":"","date":1669075200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669075200,"objectID":"079ffade0ad04d700aef8245b7366344","permalink":"https://jiangjiechen.github.io/publication/neon/","publishdate":"2022-11-22T00:00:00Z","relpermalink":"/publication/neon/","section":"publication","summary":"Generate explanations for why a statement is wrong by prompting LLMs with the correct version of it.","tags":["Text Generation","Explainable NLP","Reasoning","Constrained Generation"],"title":"Unsupervised Explanation Generation via Correct Instantiations","type":"publication"},{"authors":["Chun Zeng","Jiangjie Chen","Tianyi Zhuang","Rui Xu","Hao Yang","Ying Qin","Shimin Tao","Yanghua Xiao"],"categories":null,"content":"","date":1649376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649376000,"objectID":"8ab3aacdd0ad0e9b6677eb084309269a","permalink":"https://jiangjiechen.github.io/publication/act/","publishdate":"2022-04-08T00:00:00Z","relpermalink":"/publication/act/","section":"publication","summary":"We present ACT, an algorithm that enhances non-autoregressive translation (NAT) by improving constraint preservation and translation quality, especially for rare constraints.","tags":["Text Generation","Machine Translation","Non-autoregressive Generation","Constrained Generation"],"title":"Neighbors Are Not Strangers: Improving Non-Autoregressive Translation under Low-Frequency Lexical Constraints","type":"publication"},{"authors":null,"categories":null,"content":"For humans, intuitive inferences are made every now and then. However, it would require reasons for humans to convince others and justify themselves of their inferences or decisions. How can machines better convince humans of their predictions? The key may lie in making the right and faithful reasons for self-justification.\nWhat is human-like reasoning? What is the holy grail of machine cognition? It is easy to be right due to various spurious correlations, but it would require some actual reasoning skills to be right for the right and faithful reasons. More importantly, symbolic reasoning is too fragile to handle everyday reasoning. Can machine reasoning happen over natural language like humans do?\nExemplar papers in this theme include:\nNEON (AAAI 2023): a two-phrase, unsupervised explanation generation framework for explaining why a statement is wrong; E-KAR (Findings of ACL 2022): a benchmark for analogical reasoning with free-text rationales for both positive and negative candidate answers; LOREN (AAAI 2022): generating faithful and accurate rationales without supervision; EDUCAT (AAAI 2022): unsupervised counterfactual reasoning for imagining possible outcomes; PRobr (Findings of ACL 2021): reasoning over natural language statements via an induced graphical model. ","date":1645920000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645920000,"objectID":"7baf49d7f3da2ca373306982ae29f270","permalink":"https://jiangjiechen.github.io/projects/reasoning/","publishdate":"2022-02-27T00:00:00Z","relpermalink":"/projects/reasoning/","section":"projects","summary":"For humans, intuitive inferences are made every now and then. However, it would require reasons for humans to convince others and justify themselves of their inferences or decisions. How can machines better convince humans of their predictions? The key may lie in being right for the right and faithful reasons.","tags":["Reasoning","Explainable NLP"],"title":"Theme I - Text Reasoning, Being Right for the Right Reasons ü§î","type":"projects"},{"authors":null,"categories":null,"content":"The success of deep text generation is being held back by its non-factuality and difficulty to control. How to unleash the power of text generation models while restraining them from talking gibberish? In this research theme, we explore the potentials of text generation w.r.t. factuality and controllability.\nTo achieve these goals, one of the key perspectives is to incorporate various prior knowledge into neural models, including logical rules, templates, external knowledge bases, etc. Exemplar papers in this theme include:\nVENCE (AAAI 2023): correcting factual errors in texts with the guidance of fact verification; ACT (NAACL 2022): improving lexically constrained non-autoregressive machine translation, especially under low-frequency ones; LOREN (AAAI 2022): interpretable fact verification against trustworthy knowledge bases; FalCon (DASFAA 2022): faithful response generation with contrastive learning; KeDy (WSDM 2022) and Keep (NLPCC 2021): diversified generation guided by knowledge graphs; HedModTmpl (ACL 2019): generating faithful entity type descriptions constrained by head-modifier templates. ","date":1645920000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645920000,"objectID":"7c4dbeb6fea73f8d1da541c3f818e854","permalink":"https://jiangjiechen.github.io/projects/generation/","publishdate":"2022-02-27T00:00:00Z","relpermalink":"/projects/generation/","section":"projects","summary":"The success of deep text generation is being held back by its non-factuality. How to unleash the power of text generation models while restraining them from talking gibberish? Perhaps the first step is the identification of non-factuality.","tags":["Text Generation"],"title":"Theme II - Towards Factual, Controllable and Versatile Text Generation ü§ñ","type":"projects"},{"authors":["Jiangjie Chen","Rui Xu","Ziquan Fu","Wei Shi","Zhongqiao Li","Xinbo Zhang","Changzhi Sun","Lei Li","Yanghua Xiao","Hao Zhou"],"categories":null,"content":"","date":1645660800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645660800,"objectID":"211550703f1e48add32fce9729a51b96","permalink":"https://jiangjiechen.github.io/publication/ekar/","publishdate":"2022-02-24T00:00:00Z","relpermalink":"/publication/ekar/","section":"publication","summary":"We benchmark knowledge-intensive analogical reasoning with human-annotated explanations.","tags":["Reasoning","Analogical Reasoning","Explainable NLP","Dataset","Benchmark"],"title":"E-KAR: A Benchmark for Rationalizing Natural Language Analogical Reasoning","type":"publication"},{"authors":["Shineng Fang","Jiangjie Chen","Xinyao Shen","Yunwen Chen","Yanghua Xiao"],"categories":null,"content":"","date":1645574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645574400,"objectID":"c51fd179c47808c4dac60cc7c33c865e","permalink":"https://jiangjiechen.github.io/publication/falcon/","publishdate":"2022-01-24T00:00:00Z","relpermalink":"/publication/falcon/","section":"publication","summary":"In a practical TableQA system, response generation is a critical module to generate a natural language description of the SQL and the execution result. Due to the complex syntax of SQL and matching issues with table content, this task is prone to produce factual errors. In this paper, we propose FALCON, a FAithfuL CONtrastive generation framework to improve the factual correctness of generated responses. FALCON forces the generation model to identify examples with factual errors in the latent space during training and takes contrastive examples into consideration during inference. We also propose two new automatic metrics to further evaluate faithfulness specialized to this task. Experimental results show FALCON brings a favorable performance improvement on both automatic and human evaluation amongst various baseline methods.","tags":["Text Generation","Faithfulness and Factuality"],"title":"FalCon: A Faithful Contrastive Framework for Response Generation in TableQA Systems","type":"publication"},{"authors":["Jiangjie Chen","Qiaoben Bao","Changzhi Sun","Xinbo Zhang","Jiaze Chen","Hao Zhou","Yanghua Xiao","Lei Li"],"categories":null,"content":"","date":1645488000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645488000,"objectID":"8f66cae4e41bfb77e4da825131a2bbf8","permalink":"https://jiangjiechen.github.io/publication/loren/","publishdate":"2022-01-01T00:00:00Z","relpermalink":"/publication/loren/","section":"publication","summary":"Interpretable fact verification with phrasal decomposition and logic regularization.","tags":["Reasoning","Explainable NLP","Faithfulness and Factuality","Retrieval"],"title":"LOREN: Logic-Regularized Reasoning for Interpretable Fact Verification","type":"publication"},{"authors":["Jiangjie Chen","Chun Gan","Sijie Cheng","Hao Zhou","Yanghua Xiao","Lei Li"],"categories":null,"content":"","date":1645488000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645488000,"objectID":"57120daf6bce6cc69dc265f1d02b7c2e","permalink":"https://jiangjiechen.github.io/publication/educat/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/publication/educat/","section":"publication","summary":"Creating counterfactual story endings (what-if stories) with unsupervised editing.","tags":["Reasoning","Text Generation","Counterfactual Reasoning","Constrained Generation"],"title":"Unsupervised Editing for Counterfactual Stories","type":"publication"},{"authors":["Xinyao Shen","Jiangjie Chen","Jiaze Chen","Chun Zeng","Yanghua Xiao"],"categories":null,"content":"","date":1645401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645401600,"objectID":"fcfcb4c1f9d649a51f0cfec99b67e1ce","permalink":"https://jiangjiechen.github.io/publication/kedy/","publishdate":"2022-01-07T00:00:00Z","relpermalink":"/publication/kedy/","section":"publication","summary":"Relevant articles recommendation plays an important role in online news platforms. Directly displaying recalled articles by a search engine lacks a deep understanding of the article contents. Generating clickable queries, on the other hand, summarizes an article in various aspects, which can be henceforth utilized to better connect relevant articles. Most existing approaches for generating article queries, however, do not consider the diversity of queries or whether they are appealing enough, which are essential for boosting user experience and platform drainage. To this end, we propose a Knowledge-Enhanced Diversified QuerY Generator (KeDy), which leverages an external knowledge graph (KG) as guidance. We diversify the query generation with the information of semantic neighbors of the entities in articles. We further constrain the diversification process with entity popularity knowledge to build appealing queries that users may be more interested in. The information within KG is propagated towards more popular entities with popularity-guided graph attention. We collect a news-query dataset from the search logs of a real-world search engine. Extensive experiments demonstrate our proposed KeDy can generate more diversified and insightful related queries than several strong baselines.","tags":["Knowledge Graph","Text Generation","Diversified Generation"],"title":"Diversified Query Generation Guided with Knowledge Graph","type":"publication"},{"authors":["Qiaoben Bao","Jiangjie Chen","Linfang Liu","Jiaqing Liang","Jingping Liu","Yanghua Xiao"],"categories":null,"content":"","date":1645401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645401600,"objectID":"1541906bfaf97f0234d89ffc98140ae9","permalink":"https://jiangjiechen.github.io/publication/scope/","publishdate":"2022-01-07T00:00:00Z","relpermalink":"/publication/scope/","section":"publication","summary":"Automatic Answer span Extraction (AE) focuses on identifying key information from paragraphs that can be asked. It has been used to facilitate downstream question generation tasks or data augmentation for question answering. Current work of AE heavily relies on the annotated answer spans from Machine Reading Comprehension (MRC) datasets. However, these methods suffer from the partial annotation problem due to the annotation protocols of MRC tasks. To tackle this problem, we propose SCOPE, a Structured Context graph network with Positive-unlabeled learning. SCOPE first represents the paragraph by constructing a graph with both syntactic and semantic edges, then adopts a unified pointer network for answer span identification. SCOPE narrows the discrenpency between AE and MRC by formulating AE as a Positive-unlabeled (PU) learning problem, thus recovering more answer spans from paragraphs. To evaluate newly extracted spans without annotation, we also present an automatic metric from the perspective of question answering and text summarization, which correlates well with human judgments. Comprehensive experiments on both AE and downstream tasks demonstrate the effectiveness of our proposed framework.","tags":["Information Extraction"],"title":"Harvesting More Answer Spans from Paragraphs beyond Annotation","type":"publication"},{"authors":["Xinyao Shen","Jiangjie Chen","Yanghua Xiao"],"categories":null,"content":"","date":1634256000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634256000,"objectID":"634c990bea69abfc3318cef06290dc18","permalink":"https://jiangjiechen.github.io/publication/keep/","publishdate":"2021-07-31T00:00:00Z","relpermalink":"/publication/keep/","section":"publication","summary":"Paraphrases refer to text with different expressions conveying the same meaning, which is usually modeled as a sequence-to-sequence (Seq2Seq) learning problem. Traditional Seq2Seq models mainly concentrate on fidelity while ignoring the diversity of paraphrases. Although recent studies begin to focus on the diversity of generated paraphrases, they either adopt inflexible control mechanisms or restrict to synonyms and topic knowledge. In this paper, we propose KnowledgE-Enhanced Paraphraser (KEEP) for diversified paraphrase generation, which leverages a commonsense knowledge graph to explicitly enrich the expressions of paraphrases. Specifically, KEEP retrieves word-level and phrase-level knowledge from an external knowledge graph, and learns to choose more related ones using graph attention mechanism. Extensive experiments on benchmarks of paraphrase generation show the strengths especially in the diversity of our proposed model compared with several strong baselines.","tags":["Knowledge Graph","Text Generation","Diversified Generation"],"title":"Diversified Paraphrase Generation with Commonsense Knowledge Graph","type":"publication"},{"authors":["Changzhi Sun","Xinbo Zhang","Jiangjie Chen","Chun Gan","Yuanbin Wu","Jiaze Chen","Hao Zhou","Lei Li"],"categories":null,"content":"","date":1627603200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627603200,"objectID":"79345889a5ef69f3485f5a7f689a808b","permalink":"https://jiangjiechen.github.io/publication/probr/","publishdate":"2021-08-30T00:00:00Z","relpermalink":"/publication/probr/","section":"publication","summary":"A novel approach for joint answer prediction and proof generation.","tags":["Reasoning","Explainable NLP"],"title":"Probabilistic Graph Reasoning for Natural Proof Generation","type":"publication"},{"authors":["Jiangjie Chen","Ao Wang","Haiyun Jiang","Suo Feng","Chenguang Li","Yanghua Xiao"],"categories":null,"content":"","date":1564444800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564444800,"objectID":"e3ec980cf92193ab1e303b45dd3763aa","permalink":"https://jiangjiechen.github.io/publication/hedmodgen/","publishdate":"2019-08-30T00:00:00Z","relpermalink":"/publication/hedmodgen/","section":"publication","summary":"Building head-modifier templates to constrain entity type description generation.","tags":["Knowledge Graph","Text Generation","Constrained Generation","Faithfulness and Factuality"],"title":"Ensuring Readability and Data-fidelity using Head-modifier Templates in Deep Type Description Generation","type":"publication"},{"authors":["Jindong Chen","Ao Wang","Jiangjie Chen","Yanghua Xiao","Zhendong Chu","Jingping Liu","Jiaqing Liang","Wei Wang"],"categories":null,"content":"","date":1554681600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554681600,"objectID":"6746a8064e018e765bad8150676eb171","permalink":"https://jiangjiechen.github.io/publication/cnprobase/","publishdate":"2019-04-08T00:00:00Z","relpermalink":"/publication/cnprobase/","section":"publication","summary":"We build a large-scale Chinese Taxonomy Knowledge Graph.","tags":["Knowledge Graph","Information Extraction","Dataset"],"title":"CN-Probase: A Data-driven Approach for Large-scale Chinese Taxonomy Construction","type":"publication"}]