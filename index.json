
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Jiangjie Chen (ÈôàÊ±üÊç∑) is a final year Ph.D. candidate at Fudan University in the School of Computer Science, Shanghai, China. His interested research topics are mostly around autonomous generative agents, including (but are not limited to):\nAutonomous Generative Agents: Developing advanced methods for autonomous, trustworthy, and personalized language agents. This extends towards the exploration of their interactions with multiple agents and real environments. Cognitive Modeling in Language Models: Focusing on integrating elements from cognitive science into language models, such as the aspects of belief systems, analogical reasoning, Theory-of-Mind, etc. The goal is to augment the understanding of these agents regarding themselves and others, hence enabling them to generate more cognitively-aligned and human-like responses. Reasoning and Strategic Planning: Advancing research on equipping generative agents with human-level reasoning abilities and strategic planning capabilities. This involves designing and implementing methodologies to incorporate decision-making, counterfactual thinking, and other complex reasoning tasks in generative models. Evaluations and Simulation Environments: Establishing evaluation frameworks and creating simulation environments to assess the progress of multiple generative agents. These aim to capture the multi-faceted aspects of generative intelligence, from basic belief consistency to advanced reasoning and planning capabilities. ( Download my resum√© . Could be outdated. üò∂)\n","date":1717977600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1717977600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Jiangjie Chen (ÈôàÊ±üÊç∑) is a final year Ph.D. candidate at Fudan University in the School of Computer Science, Shanghai, China. His interested research topics are mostly around autonomous generative agents, including (but are not limited to):","tags":null,"title":"Jiangjie Chen","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"c8eba8e0a437e008b6aa7c9b720412ae","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"","tags":null,"title":"Rui Xu","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"3156f48caf016b3e3d41b61d07381407","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"","tags":null,"title":"Siyu Yuan","type":"authors"},{"authors":["Ruihan Yang","Jiangjie Chen","Yikai Zhang","Siyu Yuan","Aili Chen","Kyle Richardson","Yanghua Xiao","Deqing Yang"],"categories":null,"content":"","date":1717977600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1717977600,"objectID":"715be1bf9717a354b37188530c63824f","permalink":"https://jiangjiechen.github.io/publication/selfgoal/","publishdate":"2024-06-10T00:00:00Z","relpermalink":"/publication/selfgoal/","section":"publication","summary":"We introduce SelfGoal, an automatic approach that enhances language agents' capabilities to achieve high-level goals with limited instructions and delayed feedback by adaptively breaking down goals into practical subgoals.","tags":["Large Language Models","Planning","Agent"],"title":"SelfGoal: Your Language Agents Already Know How to Achieve High-level Goals","type":"publication"},{"authors":["Jiangjie Chen","Xintao Wang","Rui Xu","Siyu Yuan","Yikai Zhang","Wei Shi","Jian Xie","Shuang Li","Ruihan Yang","Tinghui Zhu","Aili Chen","Nianqi Li","Lida Chen","Caiyu Hu","Siye Wu","Scott Ren","Ziquan Fu","Yanghua Xiao"],"categories":null,"content":"","date":1714435200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1714435200,"objectID":"67793c9dea68de81261facea75758193","permalink":"https://jiangjiechen.github.io/publication/rplasurvey/","publishdate":"2024-04-30T00:00:00Z","relpermalink":"/publication/rplasurvey/","section":"publication","summary":"This paper surveys Role-Playing Language Agents (RPLAs) by categorizing personas, discussing their development, and examining their applications, challenges, and future directions.","tags":["Survey","Role-Playing Agent","Large Language Models","Agent"],"title":"From Persona to Personalization: A Survey on Role-Playing Language Agents","type":"publication"},{"authors":["Rui Xu","Xintao Wang","Jiangjie Chen","Siyu Yuan","Xinfeng Yuan","Jiaqing Liang","Zulong Chen","Xiaoqing Dong","Yanghua Xiao"],"categories":null,"content":"","date":1713571200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1713571200,"objectID":"8e773b476fb2e72074a9505f97013456","permalink":"https://jiangjiechen.github.io/publication/destiny/","publishdate":"2024-04-20T00:00:00Z","relpermalink":"/publication/destiny/","section":"publication","summary":"We evaluate the potential of LLMs to make decisions as literary characters, using a new dataset and improved method that enhances decision-making accuracy, with future work and resources to be shared publicly.","tags":["Large Language Models","Agent","Dataset","Role-Playing Agent"],"title":"Character is Destiny: Can Large Language Models Simulate Persona-Driven Decisions in Role-Playing?","type":"publication"},{"authors":["Xinfeng Yuan","Siyu Yuan","Yuhan Cui","Tianhe Lin","Xintao Wang","Rui Xu","Jiangjie Chen","Deqing Yang"],"categories":null,"content":"","date":1713571200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1713571200,"objectID":"fdab8eac31f7201570445b53dcf9fcf3","permalink":"https://jiangjiechen.github.io/publication/persona/","publishdate":"2024-04-20T00:00:00Z","relpermalink":"/publication/persona/","section":"publication","summary":"We propose a new approach to evaluate LLMs' understanding of fictional characters by summarizing character profiles, using a specially constructed dataset, and shows promising results for their application in role-playing agents.","tags":["Large Language Models","Agent","Dataset","Role-Playing Agent"],"title":"Evaluating Character Understanding of Large Language Models via Character Profiling from Fictional Works","type":"publication"},{"authors":["Xintao Wang","Jiangjie Chen","Nianqi Li","Lida Chen","Xinfeng Yuan","Wei Shi","Xuyang Ge","Rui Xu","Yanghua Xiao"],"categories":null,"content":"","date":1712707200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1712707200,"objectID":"cc62e6fd219244bd48438aa40d4830b0","permalink":"https://jiangjiechen.github.io/publication/surveyagent/","publishdate":"2024-04-10T00:00:00Z","relpermalink":"/publication/surveyagent/","section":"publication","summary":"We propose a novel conversational AI system that enhances researchers' literature review processes by providing personalized knowledge management, literature recommendations, and query answering through a unified platform.","tags":["Large Language Models","Agent"],"title":"SurveyAgent: A Conversational System for Personalized and Efficient Research Survey","type":"publication"},{"authors":["Siye Wu","Jian Xie","Jiangjie Chen","Tinghui Zhu","Kai Zhang","Yanghua Xiao"],"categories":null,"content":"","date":1712361600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1712361600,"objectID":"70b734dea3c4873864438bd64c1c5112","permalink":"https://jiangjiechen.github.io/publication/irrelevant/","publishdate":"2024-04-06T00:00:00Z","relpermalink":"/publication/irrelevant/","section":"publication","summary":"We study how LLMs handle irrelevant information and find they struggle with content that is semantically related but ultimately not pertinent, highlighting the limitations of current systems in filtering out such distractions.","tags":["Large Language Models","LLM Analysis","Retrieval"],"title":"How Easily do Irrelevant Inputs Skew the Responses of Large Language Models?","type":"publication"},{"authors":["Zhouhong Gu","Xiaoxuan Zhu","Haoran Guo","Lin Zhang","Yin Cai","Hao Shen","Jiangjie Chen","Zheyu Ye","Yifei Dai","Yan Gao","Yao Hu","Hongwei Feng","Yanghua Xiao"],"categories":null,"content":"","date":1710892800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1710892800,"objectID":"1505cee2a7722645562fde89df04e9a8","permalink":"https://jiangjiechen.github.io/publication/agentgroupchat/","publishdate":"2024-03-20T00:00:00Z","relpermalink":"/publication/agentgroupchat/","section":"publication","summary":"We propose a simulation to study language's influence on collective behavior by having agents engage in free chat within various narrative scenarios, with findings suggesting that greater information exchange promotes more orderly and meaningful emergent behaviors.","tags":["Agent","LLM Analysis","Simulated Environment","Large Language Models"],"title":"Agent Group Chat: An Interactive Group Chat Simulacra For Better Eliciting Collective Emergent Behavior","type":"publication"},{"authors":["Yikai Zhang","Siyu Yuan","Caiyu Hu","Kyle Richardson","Yanghua Xiao","Jiangjie Chen"],"categories":null,"content":"","date":1708041600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1708041600,"objectID":"2e81269e317f3355721db3bdb0134c2c","permalink":"https://jiangjiechen.github.io/publication/timearena/","publishdate":"2024-02-16T00:00:00Z","relpermalink":"/publication/timearena/","section":"publication","summary":"TimeArena enhances LLMs with temporal dynamics for better multitasking, showing advanced models like GPT-4 still trail behind human temporal awareness.","tags":["Large Language Models","Planning","Agent","Benchmark","Simulated Environment"],"title":"TimeArena: Shaping Efficient Multitasking Language Agents in a Time-Aware Simulation","type":"publication"},{"authors":["Xintao Wang","Yunze Xiao","Jen-tse Huang","Siyu Yuan","Rui Xu","Haoran Guo","Quan Tu","Yaying Fei","Ziang Leng","Wei Wang","Jiangjie Chen","Cheng Li","Yanghua Xiao"],"categories":null,"content":"","date":1707955200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1707955200,"objectID":"b4378676134b9a9161c3057e55e599c4","permalink":"https://jiangjiechen.github.io/publication/incharacter/","publishdate":"2024-02-15T00:00:00Z","relpermalink":"/publication/incharacter/","section":"publication","summary":"We propose InCharacter, a method using psychological scales to evaluate the personality fidelity of role-playing agents (RPAs) powered by large language models.","tags":["Large Language Models","Role-Playing Agent","Agent"],"title":"InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews","type":"publication"},{"authors":["Jiayi Fu","Xuandong Zhao","Ruihan Yang","Yuansen Zhang","Jiangjie Chen","Yanghua Xiao"],"categories":null,"content":"","date":1707868800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1707868800,"objectID":"a5cb68b89e2b4c7be934dc022feac181","permalink":"https://jiangjiechen.github.io/publication/gumbelsoft/","publishdate":"2024-02-14T00:00:00Z","relpermalink":"/publication/gumbelsoft/","section":"publication","summary":"We propose GumbelSoft to improve the diversity of text outputs from LLMs while maintaining high detectability, outperforming other watermarking methods in both aspects.","tags":["Large Language Models","Watermarking","Diversified Generation"],"title":"GumbelSoft: Diversified Language Model Watermarking via the GumbelMax-trick","type":"publication"},{"authors":["Jian Xie","Kai Zhang","Jiangjie Chen","Tinghui Zhu","Renze Lou","Yuandong Tian","Yanghua Xiao","Yu Su"],"categories":null,"content":"","date":1707004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1707004800,"objectID":"0f5cd26b09ec0ad05a49eeba64143cb9","permalink":"https://jiangjiechen.github.io/publication/travelplanner/","publishdate":"2024-02-04T00:00:00Z","relpermalink":"/publication/travelplanner/","section":"publication","summary":"We introduced TravelPlanner, a benchmark for assessing language agents' planning abilities, showing that even advanced models like GPT-4 face difficulties with complex tasks.","tags":["Large Language Models","Planning","Agent","Benchmark","Tool-Use","Simulated Environment"],"title":"TravelPlanner: A Benchmark for Real-World Planning with Language Agents","type":"publication"},{"authors":["Siyu Yuan","Kaitao Song","Jiangjie Chen","Xu Tan","Yongliang Shen","Ren Kan","Dongsheng Li","Deqing Yang"],"categories":null,"content":"","date":1705276800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1705276800,"objectID":"30a2c2d2fef66f08ca3d727f1b3bcb60","permalink":"https://jiangjiechen.github.io/publication/easytool/","publishdate":"2024-01-15T00:00:00Z","relpermalink":"/publication/easytool/","section":"publication","summary":"We proposes EASYTOOL, a method that simplifies tool documentation into concise instructions, improving tool use by language models.","tags":["Tool-Use","Agent","Large Language Models","Retrieval"],"title":"EASYTOOL: Enhancing LLM-based Agents with Concise Tool Instruction","type":"publication"},{"authors":["Jiangjie Chen","Siyu Yuan","Rong Ye","Bodhisattwa Prasad Majumder","Kyle Richardson"],"categories":null,"content":"","date":1696896000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1696896000,"objectID":"eb162672168ca28ecab0c71c36a926e5","permalink":"https://jiangjiechen.github.io/publication/aucarena/","publishdate":"2023-10-10T00:00:00Z","relpermalink":"/publication/aucarena/","section":"publication","summary":"We propose AucArena to tests LLMs in auctions, showing they can strategize but with variable success, indicating potential for enhancement.","tags":["Reasoning","Planning","Agent","LLM Analysis","Simulated Environment","Large Language Models","Benchmark"],"title":"Put Your Money Where Your Mouth Is: Evaluating Strategic Planning and Execution of LLM Agents in an Auction Arena","type":"publication"},{"authors":["Shuang Li","Jiangjie Chen","Siyu Yuan","Xinyi Wu","Hao Yang","Shimin Tao","Yanghua Xiao"],"categories":null,"content":"","date":1692576000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1692576000,"objectID":"dfaa2c04032c12f1ed9f0e5ab0685755","permalink":"https://jiangjiechen.github.io/publication/idiomkb/","publishdate":"2023-08-21T00:00:00Z","relpermalink":"/publication/idiomkb/","section":"publication","summary":"We propose a multilingual idiom KB (IdiomKB) developed using LLMs to facilitate better idiomatic translation by smaller models by retrieving idioms‚Äô figurative meanings.","tags":["Machine Translation","Large Language Models","Dataset","Constrained Generation","Retrieval"],"title":"Translate Meanings, Not Just Words: IdiomKB's Role in Optimizing Idiomatic Translation with Language Models","type":"publication"},{"authors":["Jian Xie","Kai Zhang","Jiangjie Chen","Renze Lou","Yu Su"],"categories":null,"content":"","date":1684886400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684886400,"objectID":"0d69072ca97e42cce515ea9f015b0006","permalink":"https://jiangjiechen.github.io/publication/sloth/","publishdate":"2023-05-24T00:00:00Z","relpermalink":"/publication/sloth/","section":"publication","summary":"We present the first comprehensive and controlled investigation into the behavior of large language models when encountering knowledge conflicts.","tags":["Large Language Models","LLM Analysis","Retrieval"],"title":"Adaptive Chameleon or Stubborn Sloth: Revealing the Behavior of Large Language Models in Knowledge Conflicts","type":"publication"},{"authors":["Siyu Yuan","Jiangjie Chen","Xuyang Ge","Yanghua Xiao","Deqing Yang"],"categories":null,"content":"","date":1684627200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684627200,"objectID":"0e990e9bf5e064adbbf6e10b0a4e6b5c","permalink":"https://jiangjiechen.github.io/publication/scar/","publishdate":"2023-05-21T00:00:00Z","relpermalink":"/publication/scar/","section":"publication","summary":"We propose a scientific analogical reasoning benchmark with structure abduction, SCAR, and show that large language models make reasonable scientific analogies after structure abduction.","tags":["Analogical Reasoning","Dataset","Large Language Models","Reasoning","LLM Analysis","Explainable NLP","Benchmark"],"title":"Beneath Surface Similarity: Large Language Models Make Reasonable Scientific Analogies after Structure Abduction","type":"publication"},{"authors":["Siyu Yuan","Jiangjie Chen","Changzhi Sun","Jiaqing Liang","Yanghua Xiao","Deqing Yang"],"categories":null,"content":"","date":1684022400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684022400,"objectID":"9b645b154c27bfd0ca5580f5234fadab","permalink":"https://jiangjiechen.github.io/publication/analogykb/","publishdate":"2023-05-14T00:00:00Z","relpermalink":"/publication/analogykb/","section":"publication","summary":"A million-scale analogy KB derived from existing KGs, to enable large language models to achieve analogical reasoning skills.","tags":["Analogical Reasoning","Dataset","Large Language Models","Reasoning"],"title":"AnalogyKB: Unlocking Analogical Reasoning of Language Models with A Million-scale Knowledge Base","type":"publication"},{"authors":["Siyu Yuan","Jiangjie Chen","Ziquan Fu","Xuyang Ge","Soham Shah","Charles Robert Jankowski","Yanghua Xiao","Deqing Yang"],"categories":null,"content":"","date":1682985600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682985600,"objectID":"e62dc6ae58c077e1ce8c14bd663cf183","permalink":"https://jiangjiechen.github.io/publication/coscript/","publishdate":"2023-05-02T00:00:00Z","relpermalink":"/publication/coscript/","section":"publication","summary":"We propose an over-generate-then-filter approach to improve large language models (LLMs) on constrained language planning, and use it to distill a novel constrained language planning dataset, CoScript.","tags":["Planning","Dataset","Large Language Models","Constrained Generation","Benchmark"],"title":"Distilling Script Knowledge from Large Language Models for Constrained Language Planning","type":"publication"},{"authors":["Jiangjie Chen","Wei Shi","Ziquan Fu","Sijie Cheng","Lei Li","Yanghua Xiao"],"categories":null,"content":"","date":1682985600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682985600,"objectID":"92becb6ae0e6f580cda428a3c3d8a85d","permalink":"https://jiangjiechen.github.io/publication/uncommongen/","publishdate":"2023-05-02T00:00:00Z","relpermalink":"/publication/uncommongen/","section":"publication","summary":"We find that large language models (LLMs) speak too positively about negative commonsense knowledge, which is caused by statistical shortcuts and negation reporting bias from language modeling pre-training.","tags":["Large Language Models","Constrained Generation","Reasoning","LLM Analysis","Benchmark"],"title":"Say What You Mean! Large Language Models Speak Too Positively about Negative Commonsense Knowledge","type":"publication"},{"authors":["Jiangjie Chen","Yanghua Xiao"],"categories":null,"content":"","date":1669939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669939200,"objectID":"8124f02540e4f2a1f249e2d3eab7a289","permalink":"https://jiangjiechen.github.io/publication/tcdereview/","publishdate":"2022-12-02T00:00:00Z","relpermalink":"/publication/tcdereview/","section":"publication","summary":"We briefly review the recent progress of knowledge-guided NLG, sets ten goals for future development, and envisions challenges in attaining these objectives.","tags":["Text Generation","Reasoning"],"title":"Harnessing Knowledge and Reasoning for Human-Like Natural Language Generation: A Brief Review","type":"publication"},{"authors":["Jiangjie Chen","Rui Xu","Wenxuan Zeng","Changzhi Sun","Lei Li","Yanghua Xiao"],"categories":null,"content":"","date":1669075200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669075200,"objectID":"0c1332d33b542e55786ed9851f315ce5","permalink":"https://jiangjiechen.github.io/publication/vence/","publishdate":"2022-11-22T00:00:00Z","relpermalink":"/publication/vence/","section":"publication","summary":"Fact verification guided iterative factual error correction without the supervision from corretion.","tags":["Text Generation","Faithfulness and Factuality","Constrained Generation","Retrieval"],"title":"Converge to the Truth: Factual Error Correction via Iterative Constrained Editing","type":"publication"},{"authors":["Sijie Cheng","Zhiyong Wu","Jiangjie Chen","Zhixing Li","Yang Liu","Lingpeng Kong"],"categories":null,"content":"","date":1669075200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669075200,"objectID":"079ffade0ad04d700aef8245b7366344","permalink":"https://jiangjiechen.github.io/publication/neon/","publishdate":"2022-11-22T00:00:00Z","relpermalink":"/publication/neon/","section":"publication","summary":"Generate explanations for why a statement is wrong by prompting LLMs with the correct version of it.","tags":["Text Generation","Explainable NLP","Reasoning","Constrained Generation"],"title":"Unsupervised Explanation Generation via Correct Instantiations","type":"publication"},{"authors":["Chun Zeng","Jiangjie Chen","Tianyi Zhuang","Rui Xu","Hao Yang","Ying Qin","Shimin Tao","Yanghua Xiao"],"categories":null,"content":"","date":1649376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649376000,"objectID":"8ab3aacdd0ad0e9b6677eb084309269a","permalink":"https://jiangjiechen.github.io/publication/act/","publishdate":"2022-04-08T00:00:00Z","relpermalink":"/publication/act/","section":"publication","summary":"We present ACT, an algorithm that enhances non-autoregressive translation (NAT) by improving constraint preservation and translation quality, especially for rare constraints.","tags":["Text Generation","Machine Translation","Non-autoregressive Generation","Constrained Generation"],"title":"Neighbors Are Not Strangers: Improving Non-Autoregressive Translation under Low-Frequency Lexical Constraints","type":"publication"},{"authors":null,"categories":null,"content":"For humans, intuitive inferences are made every now and then. However, it would require reasons for humans to convince others and justify themselves of their inferences or decisions. How can machines better convince humans of their predictions? The key may lie in making the right and faithful reasons for self-justification.\nWhat is human-like reasoning? What is the holy grail of machine cognition? It is easy to be right due to various spurious correlations, but it would require some actual reasoning skills to be right for the right and faithful reasons. More importantly, symbolic reasoning is too fragile to handle everyday reasoning. Can machine reasoning happen over natural language like humans do?\nExemplar papers in this theme include:\nNEON (AAAI 2023): a two-phrase, unsupervised explanation generation framework for explaining why a statement is wrong; E-KAR (Findings of ACL 2022): a benchmark for analogical reasoning with free-text rationales for both positive and negative candidate answers; LOREN (AAAI 2022): generating faithful and accurate rationales without supervision; EDUCAT (AAAI 2022): unsupervised counterfactual reasoning for imagining possible outcomes; PRobr (Findings of ACL 2021): reasoning over natural language statements via an induced graphical model. ","date":1645920000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645920000,"objectID":"7baf49d7f3da2ca373306982ae29f270","permalink":"https://jiangjiechen.github.io/projects/reasoning/","publishdate":"2022-02-27T00:00:00Z","relpermalink":"/projects/reasoning/","section":"projects","summary":"For humans, intuitive inferences are made every now and then. However, it would require reasons for humans to convince others and justify themselves of their inferences or decisions. How can machines better convince humans of their predictions? The key may lie in being right for the right and faithful reasons.","tags":["Reasoning","Explainable NLP"],"title":"Theme I - Text Reasoning, Being Right for the Right Reasons ü§î","type":"projects"},{"authors":null,"categories":null,"content":"The success of deep text generation is being held back by its non-factuality and difficulty to control. How to unleash the power of text generation models while restraining them from talking gibberish? In this research theme, we explore the potentials of text generation w.r.t. factuality and controllability.\nTo achieve these goals, one of the key perspectives is to incorporate various prior knowledge into neural models, including logical rules, templates, external knowledge bases, etc. Exemplar papers in this theme include:\nVENCE (AAAI 2023): correcting factual errors in texts with the guidance of fact verification; ACT (NAACL 2022): improving lexically constrained non-autoregressive machine translation, especially under low-frequency ones; LOREN (AAAI 2022): interpretable fact verification against trustworthy knowledge bases; FalCon (DASFAA 2022): faithful response generation with contrastive learning; KeDy (WSDM 2022) and Keep (NLPCC 2021): diversified generation guided by knowledge graphs; HedModTmpl (ACL 2019): generating faithful entity type descriptions constrained by head-modifier templates. ","date":1645920000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645920000,"objectID":"7c4dbeb6fea73f8d1da541c3f818e854","permalink":"https://jiangjiechen.github.io/projects/generation/","publishdate":"2022-02-27T00:00:00Z","relpermalink":"/projects/generation/","section":"projects","summary":"The success of deep text generation is being held back by its non-factuality. How to unleash the power of text generation models while restraining them from talking gibberish? Perhaps the first step is the identification of non-factuality.","tags":["Text Generation"],"title":"Theme II - Towards Factual, Controllable and Versatile Text Generation ü§ñ","type":"projects"},{"authors":["Jiangjie Chen","Rui Xu","Ziquan Fu","Wei Shi","Zhongqiao Li","Xinbo Zhang","Changzhi Sun","Lei Li","Yanghua Xiao","Hao Zhou"],"categories":null,"content":"","date":1645660800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645660800,"objectID":"211550703f1e48add32fce9729a51b96","permalink":"https://jiangjiechen.github.io/publication/ekar/","publishdate":"2022-02-24T00:00:00Z","relpermalink":"/publication/ekar/","section":"publication","summary":"We benchmark knowledge-intensive analogical reasoning with human-annotated explanations.","tags":["Reasoning","Analogical Reasoning","Explainable NLP","Dataset","Benchmark"],"title":"E-KAR: A Benchmark for Rationalizing Natural Language Analogical Reasoning","type":"publication"},{"authors":["Shineng Fang","Jiangjie Chen","Xinyao Shen","Yunwen Chen","Yanghua Xiao"],"categories":null,"content":"","date":1645574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645574400,"objectID":"c51fd179c47808c4dac60cc7c33c865e","permalink":"https://jiangjiechen.github.io/publication/falcon/","publishdate":"2022-01-24T00:00:00Z","relpermalink":"/publication/falcon/","section":"publication","summary":"In a practical TableQA system, response generation is a critical module to generate a natural language description of the SQL and the execution result. Due to the complex syntax of SQL and matching issues with table content, this task is prone to produce factual errors. In this paper, we propose FALCON, a FAithfuL CONtrastive generation framework to improve the factual correctness of generated responses. FALCON forces the generation model to identify examples with factual errors in the latent space during training and takes contrastive examples into consideration during inference. We also propose two new automatic metrics to further evaluate faithfulness specialized to this task. Experimental results show FALCON brings a favorable performance improvement on both automatic and human evaluation amongst various baseline methods.","tags":["Text Generation","Faithfulness and Factuality"],"title":"FalCon: A Faithful Contrastive Framework for Response Generation in TableQA Systems","type":"publication"},{"authors":["Jiangjie Chen","Qiaoben Bao","Changzhi Sun","Xinbo Zhang","Jiaze Chen","Hao Zhou","Yanghua Xiao","Lei Li"],"categories":null,"content":"","date":1645488000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645488000,"objectID":"8f66cae4e41bfb77e4da825131a2bbf8","permalink":"https://jiangjiechen.github.io/publication/loren/","publishdate":"2022-01-01T00:00:00Z","relpermalink":"/publication/loren/","section":"publication","summary":"Interpretable fact verification with phrasal decomposition and logic regularization.","tags":["Reasoning","Explainable NLP","Faithfulness and Factuality","Retrieval"],"title":"LOREN: Logic-Regularized Reasoning for Interpretable Fact Verification","type":"publication"},{"authors":["Jiangjie Chen","Chun Gan","Sijie Cheng","Hao Zhou","Yanghua Xiao","Lei Li"],"categories":null,"content":"","date":1645488000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645488000,"objectID":"57120daf6bce6cc69dc265f1d02b7c2e","permalink":"https://jiangjiechen.github.io/publication/educat/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/publication/educat/","section":"publication","summary":"Creating counterfactual story endings (what-if stories) with unsupervised editing.","tags":["Reasoning","Text Generation","Counterfactual Reasoning","Constrained Generation"],"title":"Unsupervised Editing for Counterfactual Stories","type":"publication"},{"authors":["Xinyao Shen","Jiangjie Chen","Jiaze Chen","Chun Zeng","Yanghua Xiao"],"categories":null,"content":"","date":1645401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645401600,"objectID":"fcfcb4c1f9d649a51f0cfec99b67e1ce","permalink":"https://jiangjiechen.github.io/publication/kedy/","publishdate":"2022-01-07T00:00:00Z","relpermalink":"/publication/kedy/","section":"publication","summary":"Relevant articles recommendation plays an important role in online news platforms. Directly displaying recalled articles by a search engine lacks a deep understanding of the article contents. Generating clickable queries, on the other hand, summarizes an article in various aspects, which can be henceforth utilized to better connect relevant articles. Most existing approaches for generating article queries, however, do not consider the diversity of queries or whether they are appealing enough, which are essential for boosting user experience and platform drainage. To this end, we propose a Knowledge-Enhanced Diversified QuerY Generator (KeDy), which leverages an external knowledge graph (KG) as guidance. We diversify the query generation with the information of semantic neighbors of the entities in articles. We further constrain the diversification process with entity popularity knowledge to build appealing queries that users may be more interested in. The information within KG is propagated towards more popular entities with popularity-guided graph attention. We collect a news-query dataset from the search logs of a real-world search engine. Extensive experiments demonstrate our proposed KeDy can generate more diversified and insightful related queries than several strong baselines.","tags":["Knowledge Graph","Text Generation","Diversified Generation"],"title":"Diversified Query Generation Guided with Knowledge Graph","type":"publication"},{"authors":["Qiaoben Bao","Jiangjie Chen","Linfang Liu","Jiaqing Liang","Jingping Liu","Yanghua Xiao"],"categories":null,"content":"","date":1645401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645401600,"objectID":"1541906bfaf97f0234d89ffc98140ae9","permalink":"https://jiangjiechen.github.io/publication/scope/","publishdate":"2022-01-07T00:00:00Z","relpermalink":"/publication/scope/","section":"publication","summary":"Automatic Answer span Extraction (AE) focuses on identifying key information from paragraphs that can be asked. It has been used to facilitate downstream question generation tasks or data augmentation for question answering. Current work of AE heavily relies on the annotated answer spans from Machine Reading Comprehension (MRC) datasets. However, these methods suffer from the partial annotation problem due to the annotation protocols of MRC tasks. To tackle this problem, we propose SCOPE, a Structured Context graph network with Positive-unlabeled learning. SCOPE first represents the paragraph by constructing a graph with both syntactic and semantic edges, then adopts a unified pointer network for answer span identification. SCOPE narrows the discrenpency between AE and MRC by formulating AE as a Positive-unlabeled (PU) learning problem, thus recovering more answer spans from paragraphs. To evaluate newly extracted spans without annotation, we also present an automatic metric from the perspective of question answering and text summarization, which correlates well with human judgments. Comprehensive experiments on both AE and downstream tasks demonstrate the effectiveness of our proposed framework.","tags":["Information Extraction"],"title":"Harvesting More Answer Spans from Paragraphs beyond Annotation","type":"publication"},{"authors":["Xinyao Shen","Jiangjie Chen","Yanghua Xiao"],"categories":null,"content":"","date":1634256000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634256000,"objectID":"634c990bea69abfc3318cef06290dc18","permalink":"https://jiangjiechen.github.io/publication/keep/","publishdate":"2021-07-31T00:00:00Z","relpermalink":"/publication/keep/","section":"publication","summary":"Paraphrases refer to text with different expressions conveying the same meaning, which is usually modeled as a sequence-to-sequence (Seq2Seq) learning problem. Traditional Seq2Seq models mainly concentrate on fidelity while ignoring the diversity of paraphrases. Although recent studies begin to focus on the diversity of generated paraphrases, they either adopt inflexible control mechanisms or restrict to synonyms and topic knowledge. In this paper, we propose KnowledgE-Enhanced Paraphraser (KEEP) for diversified paraphrase generation, which leverages a commonsense knowledge graph to explicitly enrich the expressions of paraphrases. Specifically, KEEP retrieves word-level and phrase-level knowledge from an external knowledge graph, and learns to choose more related ones using graph attention mechanism. Extensive experiments on benchmarks of paraphrase generation show the strengths especially in the diversity of our proposed model compared with several strong baselines.","tags":["Knowledge Graph","Text Generation","Diversified Generation"],"title":"Diversified Paraphrase Generation with Commonsense Knowledge Graph","type":"publication"},{"authors":["Changzhi Sun","Xinbo Zhang","Jiangjie Chen","Chun Gan","Yuanbin Wu","Jiaze Chen","Hao Zhou","Lei Li"],"categories":null,"content":"","date":1627603200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627603200,"objectID":"79345889a5ef69f3485f5a7f689a808b","permalink":"https://jiangjiechen.github.io/publication/probr/","publishdate":"2021-08-30T00:00:00Z","relpermalink":"/publication/probr/","section":"publication","summary":"A novel approach for joint answer prediction and proof generation.","tags":["Reasoning","Explainable NLP"],"title":"Probabilistic Graph Reasoning for Natural Proof Generation","type":"publication"},{"authors":["Jiangjie Chen","Ao Wang","Haiyun Jiang","Suo Feng","Chenguang Li","Yanghua Xiao"],"categories":null,"content":"","date":1564444800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564444800,"objectID":"e3ec980cf92193ab1e303b45dd3763aa","permalink":"https://jiangjiechen.github.io/publication/hedmodgen/","publishdate":"2019-08-30T00:00:00Z","relpermalink":"/publication/hedmodgen/","section":"publication","summary":"Building head-modifier templates to constrain entity type description generation.","tags":["Knowledge Graph","Text Generation","Constrained Generation","Faithfulness and Factuality"],"title":"Ensuring Readability and Data-fidelity using Head-modifier Templates in Deep Type Description Generation","type":"publication"},{"authors":["Jindong Chen","Ao Wang","Jiangjie Chen","Yanghua Xiao","Zhendong Chu","Jingping Liu","Jiaqing Liang","Wei Wang"],"categories":null,"content":"","date":1554681600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554681600,"objectID":"6746a8064e018e765bad8150676eb171","permalink":"https://jiangjiechen.github.io/publication/cnprobase/","publishdate":"2019-04-08T00:00:00Z","relpermalink":"/publication/cnprobase/","section":"publication","summary":"We build a large-scale Chinese Taxonomy Knowledge Graph.","tags":["Knowledge Graph","Information Extraction","Dataset"],"title":"CN-Probase: A Data-driven Approach for Large-scale Chinese Taxonomy Construction","type":"publication"}]