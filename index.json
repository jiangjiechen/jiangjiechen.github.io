[{"authors":null,"categories":null,"content":"Jiangjie Chen (ÈôàÊ±üÊç∑) is a third-year Ph.D. candidate at Fudan University (FDU) in School of Computer Science, Shanghai, China, where he is advised by Prof. Yanghua Xiao at Knowledge Works Lab. He is also currently a research intern at ByteDance AI Lab, where he works closely with Prof. Lei Li (now at UCSB), Dr. Hao Zhou and Dr. Changzhi Sun.\nHe is a zealous believer in reasoning over natural language and dedicated to make neural models right for the right reasons. His research interests mainly lie in Text Reasoning, Text Generation and Explainable NLP, and particularly the intersection of them. His previous research experiences also include Knowledge Graph Construction.\n(  Download my resum√©. Could be outdated. üò∂)\n","date":1649376000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1649376000,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Jiangjie Chen (ÈôàÊ±üÊç∑) is a third-year Ph.D. candidate at Fudan University (FDU) in School of Computer Science, Shanghai, China, where he is advised by Prof. Yanghua Xiao at Knowledge Works Lab.","tags":null,"title":"Jiangjie Chen","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"25060ab889b7f040577c7b4f5346542f","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"","tags":null,"title":"Shineng Fang","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f672c6a2b4d8465bc97e6f344d6e6164","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"","tags":null,"title":"Xinyao Shen","type":"authors"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"1485551e3c8c2ab314e5b62737315b83","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"","tags":null,"title":"Ziquan Fu","type":"authors"},{"authors":["Chun Zeng","Jiangjie Chen","Tianyi Zhuang","Rui Xu","Hao Yang","Ying Qin","Shimin Tao","Yanghua Xiao"],"categories":null,"content":"","date":1649376000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649376000,"objectID":"8ab3aacdd0ad0e9b6677eb084309269a","permalink":"https://jiangjiechen.github.io/publication/act/","publishdate":"2022-04-08T00:00:00Z","relpermalink":"/publication/act/","section":"publication","summary":"Lexically constrained neural machine translation (NMT) draws much industrial attention for its practical usage in specific domains. However, current autoregressive approaches suffer from high latency. In this paper, we focus on non-autoregressive translation (NAT) for this problem for its efficiency advantage. We identify that current constrained NAT models, which are based on iterative editing, do not handle low-frequency constraints well. To this end, we propose a plug-in algorithm for this line of work, i.e., Aligned Constrained Training (ACT), which alleviates this problem by familiarizing the model with the source-side context of the constraints. Experiments on the general and domain datasets show that our model improves over the backbone constrained NAT model in constraint preservation and translation quality, especially for rare constraints.","tags":["Text Generation","Machine Translation","Non-autoregressive Generation","Constrained Generation"],"title":"Neighbors Are Not Strangers: Improving Non-Autoregressive Translation under Low-Frequency Lexical Constraints","type":"publication"},{"authors":null,"categories":null,"content":"For humans, intuitive inferences are made every now and then. However, it would require reasons for humans to convince others and justify themselves of their inferences or decisions. How can machines better convince humans of their predictions? The key may lie in making the right and faithful reasons for self-justification.\nWhat is human-like reasoning? What is the holy grail of machine cognition? It is easy to be right due to various spurious correlations, but it would require some actual reasoning skills to be right for the right and faithful reasons. More importantly, symbolic reasoning is too fragile to handle everyday reasoning. Can machine reasoning happen over natural language like humans do?\nIn this research theme, we particularly focus on three sub-topics:\n reasoning over natural language, especially with text generation techniques; reasoning with faithful and accurate rationales; counterfactual reasoning for contrastive explanations.  Exemplar papers in this theme include:\n E-KAR (Findings of ACL 2022): a benchmark for analogical reasoning with free-text rationales for both positive and negative candidate answers; LOREN (AAAI 2022): generating faithful and accurate rationales without supervision; EDUCAT (AAAI 2022): unsupervised counterfactual reasoning for imagining possible outcomes; PRobr (Findings of ACL 2021): reasoning over natural language statements via an induced graphical model.  ","date":1645920000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645920000,"objectID":"7baf49d7f3da2ca373306982ae29f270","permalink":"https://jiangjiechen.github.io/projects/reasoning/","publishdate":"2022-02-27T00:00:00Z","relpermalink":"/projects/reasoning/","section":"projects","summary":"For humans, intuitive inferences are made every now and then. However, it would require reasons for humans to convince others and justify themselves of their inferences or decisions. How can machines better convince humans of their predictions? The key may lie in being right for the right and faithful reasons.","tags":["Reasoning","Explainable NLP"],"title":"Theme I - Explainable Text Reasoning, Being Right for the Right Reasons ü§î","type":"projects"},{"authors":null,"categories":null,"content":"The success of deep text generation is being held back by its non-factuality and difficulty to control. How to unleash the power of text generation models while restraining them from talking gibberish? In this research theme, we explore the potentials of text generation w.r.t. factuality and controllability.\nTo achieve these goals, one of the key perspectives is to incorporate various prior knowledge into neural models, including logical rules, templates, external knowledge bases, etc. Exemplar papers in this theme include:\n ACT (NAACL 2022): improving lexically constrained non-autoregressive machine translation, especially under low-frequency ones; LOREN (AAAI 2022): interpretable fact verification against trustworthy knowledge bases; FalCon (DASFAA 2022): faithful response generation with contrastive learning; KeDy (WSDM 2022) and Keep (NLPCC 2021): diversified generation guided by knowledge graphs; HedModTmpl (ACL 2019): generating faithful entity type descriptions constrained by head-modifier templates.  ","date":1645920000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645920000,"objectID":"7c4dbeb6fea73f8d1da541c3f818e854","permalink":"https://jiangjiechen.github.io/projects/generation/","publishdate":"2022-02-27T00:00:00Z","relpermalink":"/projects/generation/","section":"projects","summary":"The success of deep text generation is being held back by its non-factuality. How to unleash the power of text generation models while restraining them from talking gibberish? Perhaps the first step is the identification of non-factuality.","tags":["Text Generation"],"title":"Theme II - Towards Factual, Controllable and Versatile Text Generation ü§ñ","type":"projects"},{"authors":["Jiangjie Chen","Rui Xu","Ziquan Fu","Wei Shi","Zhongqiao Li","Xinbo Zhang","Changzhi Sun","Lei Li","Yanghua Xiao","Hao Zhou"],"categories":null,"content":"","date":1645660800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645660800,"objectID":"211550703f1e48add32fce9729a51b96","permalink":"https://jiangjiechen.github.io/publication/ekar/","publishdate":"2022-02-24T00:00:00Z","relpermalink":"/publication/ekar/","section":"publication","summary":"We benchmark knowledge-intensive analogical reasoning with human-annotated explanations.","tags":["Reasoning","Analogical Reasoning","Explainable NLP","Resources"],"title":"E-KAR: A Benchmark for Rationalizing Natural Language Analogical Reasoning","type":"publication"},{"authors":["Shineng Fang","Jiangjie Chen","Xinyao Shen","Yunwen Chen","Yanghua Xiao"],"categories":null,"content":"","date":1645574400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645574400,"objectID":"c51fd179c47808c4dac60cc7c33c865e","permalink":"https://jiangjiechen.github.io/publication/falcon/","publishdate":"2022-01-24T00:00:00Z","relpermalink":"/publication/falcon/","section":"publication","summary":"In a practical TableQA system, response generation is a critical module to generate a natural language description of the SQL and the execution result. Due to the complex syntax of SQL and matching issues with table content, this task is prone to produce factual errors. In this paper, we propose FALCON, a FAithfuL CONtrastive generation framework to improve the factual correctness of generated responses. FALCON forces the generation model to identify examples with factual errors in the latent space during training and takes contrastive examples into consideration during inference. We also propose two new automatic metrics to further evaluate faithfulness specialized to this task. Experimental results show FALCON brings a favorable performance improvement on both automatic and human evaluation amongst various baseline methods.","tags":["Text Generation","Contrastive Learning","Faithfulness"],"title":"FalCon: A Faithful Contrastive Framework for Response Generation in TableQA Systems","type":"publication"},{"authors":["Jiangjie Chen","Qiaoben Bao","Changzhi Sun","Xinbo Zhang","Jiaze Chen","Hao Zhou","Yanghua Xiao","Lei Li"],"categories":null,"content":"","date":1645488000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645488000,"objectID":"8f66cae4e41bfb77e4da825131a2bbf8","permalink":"https://jiangjiechen.github.io/publication/loren/","publishdate":"2022-01-01T00:00:00Z","relpermalink":"/publication/loren/","section":"publication","summary":"Interpretable fact verification with phrasal decomposition and logic regularization.","tags":["Reasoning","Fact Verification","Explainable NLP","Faithfulness"],"title":"LOREN: Logic-Regularized Reasoning for Interpretable Fact Verification","type":"publication"},{"authors":["Jiangjie Chen","Chun Gan","Sijie Cheng","Hao Zhou","Yanghua Xiao","Lei Li"],"categories":null,"content":"","date":1645488000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645488000,"objectID":"57120daf6bce6cc69dc265f1d02b7c2e","permalink":"https://jiangjiechen.github.io/publication/educat/","publishdate":"2021-01-01T00:00:00Z","relpermalink":"/publication/educat/","section":"publication","summary":"Creating counterfactual story endings (what-if stories) with unsupervised editing.","tags":["Reasoning","Text Generation","Counterfactual Reasoning"],"title":"Unsupervised Editing for Counterfactual Stories","type":"publication"},{"authors":["Xinyao Shen","Jiangjie Chen","Jiaze Chen","Chun Zeng","Yanghua Xiao"],"categories":null,"content":"","date":1645401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645401600,"objectID":"fcfcb4c1f9d649a51f0cfec99b67e1ce","permalink":"https://jiangjiechen.github.io/publication/kedy/","publishdate":"2022-01-07T00:00:00Z","relpermalink":"/publication/kedy/","section":"publication","summary":"Relevant articles recommendation plays an important role in online news platforms. Directly displaying recalled articles by a search engine lacks a deep understanding of the article contents. Generating clickable queries, on the other hand, summarizes an article in various aspects, which can be henceforth utilized to better connect relevant articles. Most existing approaches for generating article queries, however, do not consider the diversity of queries or whether they are appealing enough, which are essential for boosting user experience and platform drainage. To this end, we propose a Knowledge-Enhanced Diversified QuerY Generator (KeDy), which leverages an external knowledge graph (KG) as guidance. We diversify the query generation with the information of semantic neighbors of the entities in articles. We further constrain the diversification process with entity popularity knowledge to build appealing queries that users may be more interested in. The information within KG is propagated towards more popular entities with popularity-guided graph attention. We collect a news-query dataset from the search logs of a real-world search engine. Extensive experiments demonstrate our proposed KeDy can generate more diversified and insightful related queries than several strong baselines.","tags":["Knowledge Graph","Text Generation","Diversified Generation"],"title":"Diversified Query Generation Guided with Knowledge Graph","type":"publication"},{"authors":["Qiaoben Bao","Jiangjie Chen","Linfang Liu","Jiaqing Liang","Jingping Liu","Yanghua Xiao"],"categories":null,"content":"","date":1645401600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1645401600,"objectID":"1541906bfaf97f0234d89ffc98140ae9","permalink":"https://jiangjiechen.github.io/publication/scope/","publishdate":"2022-01-07T00:00:00Z","relpermalink":"/publication/scope/","section":"publication","summary":"Automatic Answer span Extraction (AE) focuses on identifying key information from paragraphs that can be asked. It has been used to facilitate downstream question generation tasks or data augmentation for question answering. Current work of AE heavily relies on the annotated answer spans from Machine Reading Comprehension (MRC) datasets. However, these methods suffer from the partial annotation problem due to the annotation protocols of MRC tasks. To tackle this problem, we propose SCOPE, a Structured Context graph network with Positive-unlabeled learning. SCOPE first represents the paragraph by constructing a graph with both syntactic and semantic edges, then adopts a unified pointer network for answer span identification. SCOPE narrows the discrenpency between AE and MRC by formulating AE as a Positive-unlabeled (PU) learning problem, thus recovering more answer spans from paragraphs. To evaluate newly extracted spans without annotation, we also present an automatic metric from the perspective of question answering and text summarization, which correlates well with human judgments. Comprehensive experiments on both AE and downstream tasks demonstrate the effectiveness of our proposed framework.","tags":["Information Extraction"],"title":"Harvesting More Answer Spans from Paragraphs beyond Annotation","type":"publication"},{"authors":["Xinyao Shen","Jiangjie Chen","Yanghua Xiao"],"categories":null,"content":"","date":1634256000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634256000,"objectID":"634c990bea69abfc3318cef06290dc18","permalink":"https://jiangjiechen.github.io/publication/keep/","publishdate":"2021-07-31T00:00:00Z","relpermalink":"/publication/keep/","section":"publication","summary":"Paraphrases refer to text with different expressions conveying the same meaning, which is usually modeled as a sequence-to-sequence (Seq2Seq) learning problem. Traditional Seq2Seq models mainly concentrate on fidelity while ignoring the diversity of paraphrases. Although recent studies begin to focus on the diversity of generated paraphrases, they either adopt inflexible control mechanisms or restrict to synonyms and topic knowledge. In this paper, we propose KnowledgE-Enhanced Paraphraser (KEEP) for diversified paraphrase generation, which leverages a commonsense knowledge graph to explicitly enrich the expressions of paraphrases. Specifically, KEEP retrieves word-level and phrase-level knowledge from an external knowledge graph, and learns to choose more related ones using graph attention mechanism. Extensive experiments on benchmarks of paraphrase generation show the strengths especially in the diversity of our proposed model compared with several strong baselines.","tags":["Knowledge Graph","Text Generation","Diversified Generation"],"title":"Diversified Paraphrase Generation with Commonsense Knowledge Graph","type":"publication"},{"authors":["Changzhi Sun","Xinbo Zhang","Jiangjie Chen","Chun Gan","Yuanbin Wu","Jiaze Chen","Hao Zhou","Lei Li"],"categories":null,"content":"","date":1627603200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627603200,"objectID":"79345889a5ef69f3485f5a7f689a808b","permalink":"https://jiangjiechen.github.io/publication/probr/","publishdate":"2021-08-30T00:00:00Z","relpermalink":"/publication/probr/","section":"publication","summary":"A novel approach for joint answer prediction and proof generation.","tags":["Reasoning","Explainable NLP"],"title":"Probabilistic Graph Reasoning for Natural Proof Generation","type":"publication"},{"authors":["Jiangjie Chen","Ao Wang","Haiyun Jiang","Suo Feng","Chenguang Li","Yanghua Xiao"],"categories":null,"content":"","date":1564444800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564444800,"objectID":"e3ec980cf92193ab1e303b45dd3763aa","permalink":"https://jiangjiechen.github.io/publication/hedmodgen/","publishdate":"2019-08-30T00:00:00Z","relpermalink":"/publication/hedmodgen/","section":"publication","summary":"Building head-modifier templates to constrain entity type description generation.","tags":["Knowledge Graph","Text Generation","Constrained Generation","Faithfulness","Taxonomy Construction"],"title":"Ensuring Readability and Data-fidelity using Head-modifier Templates in Deep Type Description Generation","type":"publication"},{"authors":["Jindong Chen","Ao Wang","Jiangjie Chen","Yanghua Xiao","Zhendong Chu","Jingping Liu","Jiaqing Liang","Wei Wang"],"categories":null,"content":"","date":1554681600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554681600,"objectID":"6746a8064e018e765bad8150676eb171","permalink":"https://jiangjiechen.github.io/publication/cnprobase/","publishdate":"2019-04-08T00:00:00Z","relpermalink":"/publication/cnprobase/","section":"publication","summary":"We build a large-scale Chinese Taxonomy Knowledge Graph.","tags":["Knowledge Graph","Information Extraction","Taxonomy Construction","Resources"],"title":"CN-Probase: A Data-driven Approach for Large-scale Chinese Taxonomy Construction","type":"publication"}]