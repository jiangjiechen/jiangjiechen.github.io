<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Jiangjie Chen</title>
    <link>https://jiangjiechen.github.io/projects/</link>
      <atom:link href="https://jiangjiechen.github.io/projects/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sun, 27 Feb 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://jiangjiechen.github.io/media/icon_hua9f9b78e35233aa477f7219cbf68418f_67044_512x512_fill_lanczos_center_3.png</url>
      <title>Projects</title>
      <link>https://jiangjiechen.github.io/projects/</link>
    </image>
    
    <item>
      <title>Theme I - Explainable Text Reasoning, Being Right for the Right Reasons ðŸ¤”</title>
      <link>https://jiangjiechen.github.io/projects/reasoning/</link>
      <pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://jiangjiechen.github.io/projects/reasoning/</guid>
      <description>&lt;p&gt;For humans, intuitive inferences are made every now and then.
However, it would require reasons for humans to convince others and justify themselves of their inferences or decisions.
How can machines better convince humans of their predictions?
The key may lie in making the right and faithful reasons for self-justification.&lt;/p&gt;
&lt;p&gt;What is human-like reasoning? What is the holy grail of machine cognition?
It is easy to be right due to various spurious correlations, but it would require some actual reasoning skills to be right for the right and faithful reasons.
More importantly, symbolic reasoning is too fragile to handle everyday reasoning.
Can machine reasoning happen over natural language like humans do?&lt;/p&gt;
&lt;p&gt;Exemplar papers in this theme include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://jiangjiechen.github.io/publication/ekar/&#34;&gt;E-KAR (Findings of ACL 2022)&lt;/a&gt;: a benchmark for analogical reasoning with free-text rationales for both positive and negative candidate answers;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jiangjiechen.github.io/publication/loren/&#34;&gt;LOREN (AAAI 2022)&lt;/a&gt;: generating faithful and accurate rationales without supervision;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jiangjiechen.github.io/publication/educat/&#34;&gt;EDUCAT (AAAI 2022)&lt;/a&gt;: unsupervised counterfactual reasoning for imagining possible outcomes;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jiangjiechen.github.io/publication/probr/&#34;&gt;PRobr (Findings of ACL 2021)&lt;/a&gt;: reasoning over natural language statements via an induced graphical model.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Theme II - Towards Factual, Controllable and Versatile Text Generation ðŸ¤–</title>
      <link>https://jiangjiechen.github.io/projects/generation/</link>
      <pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://jiangjiechen.github.io/projects/generation/</guid>
      <description>&lt;p&gt;The success of deep text generation is being held back by its non-factuality and difficulty to control.
How to unleash the power of text generation models while restraining them from talking gibberish?
In this research theme, we explore the potentials of text generation w.r.t. factuality and controllability.&lt;/p&gt;
&lt;p&gt;To achieve these goals, one of the key perspectives is to &lt;strong&gt;incorporate various prior knowledge into neural models&lt;/strong&gt;, including logical rules, templates, external knowledge bases, etc.
Exemplar papers in this theme include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;publication/act/&#34;&gt;ACT (NAACL 2022)&lt;/a&gt;: improving lexically constrained non-autoregressive machine translation, especially under low-frequency ones;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jiangjiechen.github.io/publication/loren/&#34;&gt;LOREN (AAAI 2022)&lt;/a&gt;: interpretable fact verification against trustworthy knowledge bases;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jiangjiechen.github.io/publication/falcon/&#34;&gt;FalCon (DASFAA 2022)&lt;/a&gt;: faithful response generation with contrastive learning;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jiangjiechen.github.io/publication/kedy/&#34;&gt;KeDy (WSDM 2022)&lt;/a&gt; and &lt;a href=&#34;https://jiangjiechen.github.io/publication/keep/&#34;&gt;Keep (NLPCC 2021)&lt;/a&gt;: diversified generation guided by knowledge graphs;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://jiangjiechen.github.io/publication/hedmodgen/&#34;&gt;HedModTmpl (ACL 2019)&lt;/a&gt;: generating faithful entity type descriptions constrained by head-modifier templates.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
